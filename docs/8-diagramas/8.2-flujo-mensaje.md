# 8.2 Flujo de Mensaje Completo

## Diagrama de Secuencia Detallado

```mermaid
sequenceDiagram
    participant U as Usuario WhatsApp
    participant WA as WhatsApp Servers
    participant EV as Evolution API
    participant PG as PostgreSQL
    participant RD as Redis
    participant FA as FastAPI
    participant LG as LangGraph
    participant GP1 as GPT-4 (1st call)
    participant SD as SearchDocuments
    participant OE as OpenAI Embeddings
    participant MG as MongoDB Atlas
    participant GP2 as GPT-4 (2nd call)
    participant LF as Logfire

    U->>WA: "Â¿CuÃ¡nto cuesta la matrÃ­cula?"
    WA->>EV: Entrega mensaje (WebSocket)

    EV->>PG: Verificar sesiÃ³n activa
    PG-->>EV: SesiÃ³n vÃ¡lida âœ…

    EV->>RD: Check rate limit
    RD-->>EV: OK (< 20 msg/s)

    EV->>RD: Encolar mensaje temporal
    RD-->>EV: Guardado

    EV->>FA: POST /webhook
    FA->>LF: Log: Webhook recibido

    FA->>FA: Validar estructura (Pydantic)
    FA->>FA: Parse mensaje
    FA-->>EV: 200 OK

    FA->>EV: POST /chat/sendPresence (composing)
    EV->>WA: "Escribiendo..."
    WA->>U: ðŸ’¬ Escribiendo...

    FA->>LG: ainvoke(messages, config)

    LG->>GP1: "Â¿CuÃ¡nto cuesta la matrÃ­cula?" + System Prompt
    GP1->>GP1: Analiza query
    GP1-->>LG: AIMessage(tool_calls=[search_documents])
    LF->>LF: Log: GPT-4 decidiÃ³ usar tool

    LG->>SD: search_documents(query, school)

    SD->>MG: get_documents_by_school("InformÃ¡tica")
    MG-->>SD: [Reglamento Pagos, Calendario, ...]

    SD->>GP1: Seleccionar mejor documento
    GP1-->>SD: "Reglamento de Pagos 2024"
    LF->>LF: Log: Documento seleccionado

    SD->>OE: aembed_query("costo matrÃ­cula")
    OE-->>SD: Vector [1536 dims]

    SD->>MG: $vectorSearch (documento + embedding)
    MG->>MG: Comparar vectores (cosine similarity)
    MG-->>SD: Top 5 pÃ¡ginas (score > 0.8)
    LF->>LF: Log: Vector Search completado

    SD->>GP1: Generar respuesta (query + pÃ¡ginas)
    GP1-->>SD: "La matrÃ­cula cuesta S/ 350..."
    LF->>LF: Log: Respuesta generada

    SD-->>LG: ToolMessage(content="S/ 350...")

    LG->>GP2: Full context (HumanMessage + AIMessage + ToolMessage)
    GP2->>GP2: Genera respuesta final
    GP2-->>LG: AIMessage("SegÃºn el Reglamento...")
    LF->>LF: Log: Respuesta final generada

    LG-->>FA: {"messages": [..., AIMessage]}

    par Enviar respuesta + Marcar leÃ­do
        FA->>EV: POST /message/sendText
        EV->>RD: Encolar mensaje
        RD-->>EV: OK
        EV->>WA: Enviar mensaje
        WA->>U: "SegÃºn el Reglamento de Pagos..."

    and
        FA->>EV: POST /chat/markMessageAsRead
        EV->>WA: Marcar como leÃ­do
        WA->>U: âœ“âœ“ (doble check azul)
    end

    FA->>LF: Log: Mensaje enviado exitosamente
    FA-->>FA: Return {"status": "processed"}
```

---

## Tiempos de EjecuciÃ³n

```
1. Usuario â†’ WhatsApp â†’ Evolution API: ~100ms
2. Evolution API (verificaciones): ~50ms
3. Webhook â†’ FastAPI: ~20ms
4. FastAPI validaciÃ³n + parsing: ~10ms
5. Send presence (typing): ~100ms
6. LangGraph - chat (1st): ~800ms
   â””â”€ GPT-4 tool call: ~750ms
7. LangGraph - tools: ~2500ms
   â”œâ”€ Get documents: ~50ms
   â”œâ”€ Select document: ~800ms
   â”œâ”€ Generate embedding: ~50ms
   â”œâ”€ Vector Search: ~100ms
   â””â”€ Generate answer: ~1500ms
8. LangGraph - chat (2nd): ~900ms
   â””â”€ GPT-4 final response: ~850ms
9. Send message + Mark as read: ~100ms

TOTAL: ~4.6 segundos (usuario ve respuesta)
```

---

## Eventos CrÃ­ticos

### 1. VerificaciÃ³n de SesiÃ³n
```
Evolution API â†’ PostgreSQL
Si falla â†’ No se procesa el mensaje
```

### 2. Rate Limiting
```
Evolution API â†’ Redis
Si excede â†’ Mensaje rechazado
```

### 3. Webhook RecepciÃ³n
```
Evolution API â†’ FastAPI
Debe responder < 5 segundos
```

### 4. Vector Search
```
MongoDB Atlas
Performance crÃ­tica: ~100ms
```

### 5. OpenAI Calls
```
3 llamadas por consulta:
- Tool call decision: ~750ms
- Document selection: ~800ms
- Final response: ~850ms
```

---

## Puntos de Fallo

### 1. Evolution API CaÃ­do
```
Usuario â†’ WhatsApp â†’ âŒ Evolution API
SoluciÃ³n: Railway auto-restart
```

### 2. FastAPI CaÃ­do
```
Evolution API â†’ âŒ FastAPI
SoluciÃ³n: Railway auto-restart + health checks
```

### 3. MongoDB Lento/CaÃ­do
```
SearchDocuments â†’ âŒ MongoDB
SoluciÃ³n: Timeout + retry + error message al usuario
```

### 4. OpenAI Rate Limit
```
LangGraph â†’ âŒ OpenAI (429 Too Many Requests)
SoluciÃ³n: Exponential backoff + queue
```

---

## Optimizaciones

### 1. CachÃ© de Documentos
```python
# Cache lista de documentos por escuela
# Reduce ~50ms por consulta
```

### 2. Parallel Execution
```python
# Send message + Mark as read en paralelo
# Reduce ~100ms
```

### 3. Streaming
```python
# (Futuro) Stream respuesta de GPT-4
# Usuario ve respuesta progresivamente
```

---

## Observabilidad

Cada paso se registra en Logfire:

```
[WEBHOOK] Received: messages.upsert (instance: sciencebot)
[PARSE] Message from 51999999999: "Â¿CuÃ¡nto cuesta..."
[LANGGRAPH] Invoking agent
[GPT-4] Tool call: search_documents
[SEARCH] Get documents: IngenierÃ­a InformÃ¡tica
[SEARCH] Selected: Reglamento de Pagos 2024
[SEARCH] Vector Search: 5 results (avg score: 0.89)
[GPT-4] Generated answer (tokens: 150)
[GPT-4] Final response (tokens: 100)
[EVOLUTION] Message sent successfully
[DONE] Total time: 4.6s
```

---

**Volver al Ã­ndice**: [../README.md](../README.md)
