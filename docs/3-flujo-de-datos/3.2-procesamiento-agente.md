# 3.2 Procesamiento del Agente

## Inicializaci√≥n del Grafo

El grafo de LangGraph se inicializa **una sola vez** al arrancar el servidor:

```python
# app/lifespan.py

@asynccontextmanager
async def lifespan(app: FastAPI):
    # üü¢ STARTUP
    print("[INFO] Initializing LangGraph agent...")
    graph = get_graph()  # Compilar el grafo
    app.state.science_bot_graph = graph  # Guardar en app.state
    print("[INFO] Agent ready!")

    yield

    # üî¥ SHUTDOWN
    print("[INFO] Shutting down...")
```

**Ventaja**: El grafo compilado est√° listo para usarse en cada request, sin overhead de compilaci√≥n.

---

## Invocaci√≥n del Agente

```python
# app/routes/webhook.py

@router.post("/webhook")
async def handle_webhook(webhook: WebhookPayload, request: Request):
    # 1. Parsear mensaje
    parsed = evolution_service.parse_webhook_message(webhook)

    if not parsed:
        return {"status": "ignored"}

    # 2. Obtener grafo desde app.state
    graph = request.app.state.science_bot_graph

    # 3. Invocar agente
    response = await graph.ainvoke(
        input={
            "messages": [
                HumanMessage(content=parsed.text)
            ]
        },
        config={
            "phone_number": parsed.phone_number
        }
    )

    # 4. Extraer respuesta final
    final_message = response["messages"][-1].content

    # 5. Enviar respuesta
    await evolution_service.send_message(
        phone_number=parsed.phone_number,
        message=final_message,
        instance_name="sciencebot-production"
    )

    return {"status": "processed"}
```

---

## Flujo del Grafo: chat ‚Üí tools ‚Üí chat

```mermaid
stateDiagram-v2
    [*] --> chat: HumanMessage

    state chat {
        [*] --> LoadContext
        LoadContext --> InvokeGPT
        InvokeGPT --> [*]
    }

    chat --> should_continue: AIMessage

    state should_continue <<choice>>
    should_continue --> tools: Has tool_calls
    should_continue --> [*]: No tool_calls

    state tools {
        [*] --> ExecuteTool
        ExecuteTool --> search_documents
        search_documents --> [*]
    }

    tools --> chat: ToolMessage
```

---

## Nodo 1: chat (Primera Invocaci√≥n)

### Input
```python
{
    "messages": [
        HumanMessage(content="¬øCu√°nto cuesta la matr√≠cula en Inform√°tica?")
    ]
}
```

### Procesamiento
```python
async def chat(state: InputState, config: RunnableConfig):
    # 1. Cargar contexto
    context = Context.from_config(config)  # phone_number

    # 2. Modelo con tools
    model = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    model_with_tools = model.bind_tools(tools=TOOLS, strict=True)

    # 3. System prompt
    prompt = ChatPromptTemplate.from_messages([
        ("system", get_system_prompt(phone_number=context.phone_number)),
        MessagesPlaceholder(variable_name="messages"),
    ])

    # 4. Invocar
    response = await (prompt | model_with_tools).ainvoke(
        input={"messages": state.messages}
    )

    return {"messages": [response]}
```

### Output (GPT-4 decide usar tool)
```python
AIMessage(
    content="",
    tool_calls=[{
        "name": "search_documents",
        "args": {
            "query": "costo de matr√≠cula",
            "school": "INFORMATICA"
        }
    }]
)
```

---

## Conditional Edge: should_continue

```python
async def should_continue(state: OverallState) -> Literal["tools", "__end__"]:
    last_message = state.messages[-1]

    if last_message.tool_calls:
        return "tools"  # ‚Üí Ejecutar herramientas

    return "__end__"    # ‚Üí Finalizar
```

**Decisi√≥n**: Como el mensaje tiene `tool_calls`, el flujo contin√∫a hacia el nodo `tools`.

---

## Nodo 2: tools (Ejecuci√≥n de Herramientas)

```python
# ToolNode ejecuta autom√°ticamente las tools
from langgraph.prebuilt import ToolNode

graph_builder.add_node(node="tools", action=ToolNode(tools=TOOLS))
```

### Input (del mensaje anterior)
```python
tool_calls=[{
    "name": "search_documents",
    "args": {
        "query": "costo de matr√≠cula",
        "school": "INFORMATICA"
    }
}]
```

### Procesamiento
```python
# ToolNode internamente hace:
1. Encuentra la funci√≥n search_documents
2. Extrae args
3. Ejecuta: result = await search_documents(query="costo de matr√≠cula", school="INFORMATICA")
4. Crea ToolMessage con el resultado
```

### Output
```python
ToolMessage(
    content='{"success": true, "message": "Seg√∫n el Reglamento de Pagos 2024, el costo de matr√≠cula es de S/ 350 soles..."}',
    tool_call_id="call_abc123"
)
```

---

## Nodo 3: chat (Segunda Invocaci√≥n)

Despu√©s de ejecutar la tool, el grafo vuelve al nodo `chat`:

### Input (estado actualizado)
```python
{
    "messages": [
        HumanMessage(content="¬øCu√°nto cuesta la matr√≠cula en Inform√°tica?"),
        AIMessage(content="", tool_calls=[...]),
        ToolMessage(content="...S/ 350 soles...")
    ]
}
```

### Procesamiento
GPT-4 ahora tiene todo el contexto:
- Pregunta original del usuario
- Decisi√≥n de usar search_documents
- Resultado de la b√∫squeda

```python
# GPT-4 genera respuesta final combinando toda la info
response = await model.ainvoke([
    SystemMessage("Eres un asistente..."),
    HumanMessage("¬øCu√°nto cuesta la matr√≠cula en Inform√°tica?"),
    AIMessage(tool_calls=[...]),
    ToolMessage("...S/ 350 soles...")
])
```

### Output (respuesta final)
```python
AIMessage(
    content="Seg√∫n el Reglamento de Pagos 2024, el costo de matr√≠cula para Ingenier√≠a Inform√°tica es de S/ 350 soles. Este pago debe realizarse al inicio de cada semestre acad√©mico."
)
```

---

## Conditional Edge: should_continue (segunda vez)

```python
last_message = state.messages[-1]

if last_message.tool_calls:  # No hay tool_calls esta vez
    return "tools"

return "__end__"  # ‚úÖ Finalizar grafo
```

**Decisi√≥n**: Como no hay m√°s `tool_calls`, el grafo finaliza y retorna el estado completo.

---

## Estado Final

```python
{
    "messages": [
        HumanMessage(content="¬øCu√°nto cuesta la matr√≠cula en Inform√°tica?"),
        AIMessage(content="", tool_calls=[{...}]),
        ToolMessage(content="...S/ 350 soles..."),
        AIMessage(content="Seg√∫n el Reglamento de Pagos 2024, el costo de matr√≠cula para Ingenier√≠a Inform√°tica es de S/ 350 soles. Este pago debe realizarse al inicio de cada semestre acad√©mico.")
    ]
}
```

---

## Extracci√≥n de la Respuesta Final

```python
# En el webhook handler
response = await graph.ainvoke(...)

# Extraer √∫ltimo mensaje (respuesta del AI)
final_message = response["messages"][-1].content

# "Seg√∫n el Reglamento de Pagos 2024, el costo de matr√≠cula..."
```

---

## Casos de Uso

### Caso 1: Sin Tool (Respuesta Directa)

**Input**: "Hola"

**Flujo**:
```
chat ‚Üí AIMessage("¬°Hola! Soy ScienceBot...")
‚Üí should_continue ‚Üí __end__
```

**Resultado**: Una sola invocaci√≥n a GPT-4, sin herramientas.

### Caso 2: Con Tool (B√∫squeda)

**Input**: "¬øCu√°nto cuesta la matr√≠cula?"

**Flujo**:
```
chat ‚Üí AIMessage(tool_calls=[...])
‚Üí should_continue ‚Üí tools
‚Üí tools ‚Üí ToolMessage(...)
‚Üí chat ‚Üí AIMessage("Seg√∫n el Reglamento...")
‚Üí should_continue ‚Üí __end__
```

**Resultado**: Dos invocaciones a GPT-4 + una ejecuci√≥n de tool.

---

## Diagrama de Secuencia Completo

```mermaid
sequenceDiagram
    participant W as Webhook Handler
    participant G as LangGraph
    participant C1 as chat (1st)
    participant GPT1 as GPT-4 (1st)
    participant SC as should_continue
    participant T as tools
    participant SD as search_documents
    participant C2 as chat (2nd)
    participant GPT2 as GPT-4 (2nd)

    W->>G: ainvoke(HumanMessage)
    G->>C1: Invoke
    C1->>GPT1: System prompt + HumanMessage
    GPT1-->>C1: AIMessage(tool_calls=[...])
    C1-->>G: {"messages": [AIMessage]}

    G->>SC: Check tool_calls
    SC-->>G: "tools"

    G->>T: Execute
    T->>SD: search_documents(query, school)
    SD-->>T: SearchDocumentsResponse
    T-->>G: {"messages": [ToolMessage]}

    G->>C2: Invoke
    C2->>GPT2: Full context (HumanMessage + AIMessage + ToolMessage)
    GPT2-->>C2: AIMessage("Seg√∫n el Reglamento...")
    C2-->>G: {"messages": [AIMessage]}

    G->>SC: Check tool_calls
    SC-->>G: "__end__"

    G-->>W: Final state
```

---

## Performance

```
Tiempos t√≠picos:

1. chat (1st invocation): ~800ms
   - GPT-4o-mini es r√°pido
   - Decide usar tool

2. tools (search_documents): ~2500ms
   - Get documents: 50ms
   - Select document: 800ms (GPT-4)
   - Vector Search: 100ms
   - Generate answer: 1500ms (GPT-4)

3. chat (2nd invocation): ~1000ms
   - Genera respuesta final

TOTAL: ~4.3 segundos
```

---

## Pr√≥ximos Pasos

- **[3.3 B√∫squeda de Documentos](./3.3-busqueda-documentos.md)**: Detalle del pipeline de b√∫squeda
- **[3.4 Respuesta al Usuario](./3.4-respuesta-usuario.md)**: Env√≠o via Evolution API

**Volver al √≠ndice**: [../README.md](../README.md)
