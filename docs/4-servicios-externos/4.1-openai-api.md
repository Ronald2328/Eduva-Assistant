# 4.1 OpenAI API

## Modelos Usados

### 1. GPT-4o-mini (Chat Completion)

**Uso**: Generación de respuestas conversacionales

```python
model = ChatOpenAI(
    model="gpt-4o-mini",
    api_key=SecretStr(settings.OPENAI_API_KEY),
    max_completion_tokens=1000,
    temperature=0
)
```

**Características**:
- Modelo económico (~60% más barato que GPT-4)
- Rápido (~800ms por request)
- Buen balance calidad/precio
- Soporta function calling

### 2. text-embedding-3-small (Embeddings)

**Uso**: Vectorización de textos para búsqueda semántica

```python
embedding = OpenAIEmbeddings(
    api_key=SecretStr(settings.OPENAI_API_KEY),
    model="text-embedding-3-small"
)

vector = await embedding.aembed_query("¿Cuánto cuesta la matrícula?")
# Retorna: [0.123, -0.456, 0.789, ...] (1536 dimensiones)
```

---

## Diferencia entre Chat y Embeddings

| Aspecto | Chat Completion | Embeddings |
|---------|----------------|------------|
| **Propósito** | Generar texto | Vectorizar texto |
| **Input** | Prompt | Texto a vectorizar |
| **Output** | Texto generado | Vector numérico |
| **Modelo** | gpt-4o-mini | text-embedding-3-small |
| **Costo** | $0.15 / 1M tokens | $0.02 / 1M tokens |
| **Latencia** | ~800ms | ~50ms |

---

## Tokens y Costos

### Cálculo de Tokens

```python
# Aproximación: 1 token ≈ 0.75 palabras (inglés)
# Español: 1 token ≈ 0.6 palabras

texto = "¿Cuánto cuesta la matrícula?"
# ~7 tokens

respuesta = "Según el Reglamento de Pagos 2024..."
# ~50 tokens
```

### Costos (GPT-4o-mini)

```python
# Input: $0.15 / 1M tokens
# Output: $0.60 / 1M tokens

# Consulta típica:
# - System prompt: 200 tokens
# - User query: 10 tokens
# - Tool context: 500 tokens
# - AI response: 100 tokens

# Costo por consulta:
# Input: (200 + 10 + 500) * $0.15 / 1M = $0.0001065
# Output: 100 * $0.60 / 1M = $0.00006
# TOTAL: ~$0.00017 por consulta
```

### Costos (text-embedding-3-small)

```python
# $0.02 / 1M tokens

# Query: 10 tokens
# Costo: 10 * $0.02 / 1M = $0.0000002

# 1000 consultas = $0.0002
```

---

## Configuración de Temperature

```python
temperature=0  # Determinístico, sin creatividad
```

**Valores**:
- `0`: Siempre la misma respuesta (determinístico)
- `0.5`: Balance entre creatividad y consistencia
- `1.0`: Creativo, variado
- `2.0`: Muy aleatorio

**¿Por qué 0?**: Para un asistente académico, necesitamos respuestas consistentes y factuales.

---

## Structured Outputs (Tool Calling)

### Bind Tools con Strict Mode

```python
model_with_tools = model.bind_tools(tools=TOOLS, strict=True)
```

**`strict=True`**: OpenAI garantiza que el output sigue exactamente el schema.

### Schema de la Tool

```python
@tool
async def search_documents(
    query: str,
    school: SchoolEnum,
) -> SearchDocumentsResponse:
    """Busca información en documentos."""
    pass
```

### OpenAI Retorna

```json
{
  "tool_calls": [
    {
      "name": "search_documents",
      "arguments": {
        "query": "costo matrícula",
        "school": "INFORMATICA"
      }
    }
  ]
}
```

**Ventaja**: No hay errores de parsing, el schema es respetado.

---

## Recursos Adicionales

- [OpenAI Pricing](https://openai.com/pricing)
- [OpenAI API Docs](https://platform.openai.com/docs)

**Volver al índice**: [../README.md](../README.md)
